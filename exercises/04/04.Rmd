---
title: "EX 04 - confidense intervals and MLE motivation"
author: "Afek Adler"
date: "`r Sys.Date()`"
output:
  html_document :  
    number_sections: TRUE
---

Last excercise we did:

  * Cover methods for point estimattion
  * Get to know dplyr package
  * Developed a feeling for bayesian estimation

Today we will:

  * Talk on interavel estimation
  * Go throw a reminder on MLE with a motivation for machine learning (a mixture process)


# Interval Estimation

## The student's t distribution 
t distribution is used to model the expected values of a **small** sample from a population that is distributed noraml with unknown variance.
As N increases, t distribution is getting closer and closer to the normal distribution.

Lemma: 
$\frac{\bar{X}-\mu}{S / \sqrt{n}}$ is t- distributed.

[Student t dist vs normal dist as function of n](https://rpsychologist.com/d3/tdist/)

## confidense interval

In statistics, a confidence interval (CL) is a type of interval estimate, computed from the statistics of the observed data, that might contain the true value of an unknown population parameter. The interval has an associated confidence level, or coverage that, loosely speaking, quantifies the level of confidence that the deterministic parameter is captured by the interval. More strictly speaking, the confidence level represents the frequency (i.e. the proportion) of possible confidence intervals that contain the true value of the unknown population parameter. **In other words, if confidence intervals are constructed using a given confidence level from an infinite number of independent sample statistics, the proportion of those intervals that contain the true value of the parameter will be equal to the confidence level**. [wiki](https://en.wikipedia.org/wiki/Confidence_interval)

There are one sided and two sided confidense intervals.

### confidense interval for the mean based on n samples: 
Based on our assumptions we get a different distribution of the sample, after we figure out how the sample is distributed computing the confidense interval is straighforward. for example, for the mean - 

\[\mu \in(\bar{X}-\#of\_std_\_for\_confidense\_level\_\alpha*std,\bar{X}+\#of\_std_\_for\_confidense\_level\_\alpha*std) \]
1.When variance is known and the population is assumed to be distributer notmal or n is "big" (n > 30), the sample mean is distributed
\[\mathcal{N}\left(\bar{X} , \frac{\sigma^{2}}{n}\right)\]
So  a two sided confidense interval is:
\[\mu \in\left(\bar{X}-Z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}},\bar{X}+Z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right)\]
2 .When variance is **not** known and n is "big" (n > 30), the sample mean is distributed
\[\mathcal{N}\left(\bar{X} , \frac{\hat{\sigma}^{2}}{n}\right)\]
3. When variance is **not** known and n is **not** "big" (n <= 30), the sample mean is distributed
\[\mathcal{t_{n-1}}\left(\bar{X} , \frac{\hat{\sigma}^{2}}{n}\right)\] 

Reminder:
\[\hat{\sigma}^{2} = S^2 =\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}{n-1}=\frac{\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}}{n-1} \]

For your own understanding, for each case, derive the one sided and two sided confidense interval at home.

* We have seen in the lecture alse CL for the $\sigma^2$, and we will encounter it at HW2 as well.

### confidense interval for the proportion based on n samples
if n is large enough, the proportion is distributed:
\[\mathcal{N}\left(\hat{p} , \frac{\hat{p}\hat{q}}{n}\right)\]

**Q1:**
An online advertising company is doing an ab testing for a new advertisement and wants to model a confidense interval for the click thorogh rate (CTR) of a givent test such that it's confidense interval will be smaller than 5%  in confidense level of 95% . What is the minmum number of sample for this purpose?

\[0.05 = Length \geq 2*Z_{1-\frac{\alpha}{2}}*\sqrt{\frac{\hat{p}\hat{q}}{n}} =  2*Z_.975*\sqrt{\frac{\hat{p}\hat{q}}{n}} \Rightarrow\]
\[=  2*1.96*\sqrt{\frac{\hat{p}\hat{q}}{n}} \Rightarrow\ 0.01275 \geq \sqrt{\frac{\hat{p}\hat{q}}{n}} \Rightarrow\ n \geq \frac{\hat{p}\hat{q}}{0.00016} \Rightarrow\]
\[n \geq  1562.5 = \frac{0.5*0.5}{0.00016} \geq \frac{\hat{p}\hat{q}}{0.00016}\]
because \[ p(1-p) \leq 0.5*0.5  \ \forall p \in \{0,1\} \]

If we know for example that the CTR is bounded by 4% than:
\[n \geq  240 = \frac{0.5*0.5}{0.00016} \geq \frac{\hat{p}\hat{q}}{0.00016}\]

# CL Verification
Let's verify that indeed when we bouild CI (and..our assumptions are correct) than $1-\alpha$ our parameter is inside the CI:  
``` {r confidense interval}
miu = 10
sigma = 3
n = 10
alpha = 0.1
N_tests <- 10000
counter <- 0
error = qnorm(1-alpha/2)*(sigma/sqrt(n))
for (i in 1:N_tests)
  {sample = rnorm(n,miu,sigma)
  sample_mean <- mean(sample)
  left <- sample_mean-error 
  right <- sample_mean + error
  between <- (left <= miu) & (miu <= right)
  counter <- counter+between}
print(counter/N_tests)
```

# MLE motivation

Motivation example: solving MLE with the expectation maximization algorithm for GMM
## MLE for Human Hight

Assume that we are given data of human height but we don't know their gender.


``` {r}
photo_path <- 'https://ourworldindata.org/uploads/2019/06/distribution-1.png'
destination_path <- 'heights.jpg'
download.file(photo_path,destination_path, mode = 'wb')
```

```{r human height, echo=FALSE, fig.cap="Distribution of height by Gender", out.width = '75%'}
knitr::include_graphics(destination_path)
```

Let's generate a sample:

```{r  sample generation}
library(tidyverse)
set.seed(42)
n_samples = 500

male_avg_h = 178.4
male_std_h = 7.6

female_avg_h = 154  # 164.7 
female_std_h = 7.1

male_h <- rnorm(n_samples, male_avg_h, male_std_h)
female_h <- rnorm(n_samples, female_avg_h, female_std_h)

# a plot using ggplot
all_h <- tibble(h = c(male_h,female_h))

ggplot(all_h, aes(h)) + 
  geom_density(fill = "red", color = "blue") + 
  ggtitle("Human Height [cm]") +
  labs(caption = "n=1000")
```


Let's assume we know only the heights vector but we don't know the genders at all.
We call the gender a hidden variable because we never observe it.
let's consider a model to generate this vector:

For each sample:

  1. Sample a gender: \[ Z \in \{Female,Male\} \]
NOTE : in our example z is derived from a Bernoulli distribution but the general case is Categorical distribution.
for simplicity lets note: 
 \[ p(Z = Male) = p ,p(Z = Female) = 1-p\]
  2. After we know a gender we need to sample a height - let's assume: 
\[ p(X|Z) = \mathcal{N}\left(\mu_z , \sigma_z^{2}\right)\]

NOTE : it means that there are parameters that describe the height of Males and different parametrs that describe the height Females but their both from normal distribution.

Therefore we can deduce:
\[ p(X = x ,Z= z ) = p(Z=z)* p(X = x|Z= z) \]

Suppose we know the genders in our sample, finding the parameters is than trivial:
\[ \hat{p} = NumberOfMalesInSample/SampleSize \]
\[ \hat{\mu}_{Male} = AvgMaleHeight \]
\[ \hat{\sigma}_{Male} = SigmaMaleHeight \]


If we see a height and we want to know whether its a Female or Male, we can do it in a probabalistic manner, given that we know the parameters of the model:
\[ p(Z= z|X = x) = p(X = x ,Z= z )/p(X=x)\]
This is called soft clustering.
and if we would have to guess a class (gender), than we would guess:
\[ Z_i = argmax_z p(Z= z|X = x_i)\]
This is called Hard clustering.
So if we know the parameters we "know" Z and if we know Z we know the parameters.


**But we don't know them..** MLE to the rescue!
Let's define the likelihood of one sample:
\[ p(X = x) = p(Z=Male)* p(X = x|Z=Male) + p(Z=Female)* p(X = x|Z=Fenale)\]
And the complete Data liklihood:
\[\prod_{i}^{n}  p(X = x_i) = \prod_{i}^{n} (p(Z=Male)* p(X = x_i|Z=Male) + p(Z=Female)* p(X = x_i|Z=Fenale))\]
But you will find that this term is impossible to optimize analytically. 
Luckily, there are ways to solve this problem (heuristically):
  
  1. Expectaion maximization algorithm - There exist a lower bound on the log likelihood of the data and it's possible to optimize it iteratively, those improving the likelihood. But we are not guaranteed a global maxima.
  2. Gradient descent (with a a constrain on each sigma being positive semi definite).
  
We will not cover this terms in this course but you will meet the gradient descent algorithm in the future.


Resources : 

* [Most detailed explanations](http://cs229.stanford.edu/notes/cs229-notes8.pdf)
* [Example in 2d](http://www.ee.bgu.ac.il/~haimp/ml/lectures/lec2/lec2.pdf)
* [Video](https://www.youtube.com/watch?v=Rkl30Fr2S38)


Q: 

* What happens if there are more females? 
* What happens if we care more about false positives/false negatives?

A : \

** Wait For Intro to Machine Learning Course! **
