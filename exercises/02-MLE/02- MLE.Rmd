---
title: "02 - Maximum Likelihood Estimators"
author: "Afek Adler"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Today we are going to go over MLE (Maximum Likelihood Estimation):

   1. Motivation example: solving MLE with the expectation maximization algorithm for GMM
   2. Simple examples with the the binomial distribution/poisson/uniform/noraml from wiki


#### From Wikipedia

In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.

If the likelihood function is differentiable, the derivative test for determining maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved explicitly; for instance, the ordinary least squares estimator maximizes the likelihood of the linear regression model. Under most circumstances, however, numerical methods will be necessary to find the maximum of the likelihood function. 

### MLE for Human Hight

Assume that we are given data of human height but we don't know their gender.


``` {r}
photo_path <- 'https://ourworldindata.org/uploads/2019/06/distribution-1.png'
destination_path <- 'heights.jpg'
download.file(photo_path,destination_path, mode = 'wb')
```

```{r human height, echo=FALSE, fig.cap="Distribution of height by Gender", out.width = '75%'}
knitr::include_graphics(destination_path)
```

Let's generate a sample:

```{r  sample generation}
set.seed(42)
n_samples = 500

male_avg_h = 178.4
male_std_h = 7.6

female_avg_h = 154  # 164.7 
female_std_h = 7.1

male_h <- rnorm(n_samples, male_avg_h, male_std_h)
female_h <- rnorm(n_samples, female_avg_h, female_std_h)

# a plot using ggplot
library(tidyverse)
all_h <- tibble(h = c(male_h,female_h))

ggplot(all_h, aes(h)) + 
  geom_density(fill = "red", color = "blue") + 
  ggtitle("Human Height [cm]") +
  labs(caption = "n=1000")

# this can also be generated via base R plots, but we highly recommend the use of ggplot2 for producing plots
# Kernel Density Plot
d <- density(all_h$h)

plot(d, main = "Human Height (CM)" )
polygon(d, col="red", border="blue") 
```



Let's assume we know only the heights vector but we don't know the genders at all.
We call the gender a hidden variable because we never observe it.
let's consider a model to generate this vector:

For each sample:

  1. Sample a gender: \[ Z \in \{Female,Male\} \]
NOTE : in our example z is derived from a Bernoulli distribution but the general case is Categorical distribution.
for simplicity lets note: 
 \[ p(Z = Male) = p ,p(Z = Female) = 1-p\]
  2. After we know a gender we need to sample a height - let's assume: 
\[ p(X|Z) = \mathcal{N}\left(\mu_z , \sigma_z^{2}\right)\]

NOTE : it means that there are parameters that describe the height of Males and different parametrs that describe the height Females but their both from normal distribution.

Therefore we can deduce:
\[ p(X = x ,Z= z ) = p(Z=z)* p(X = x|Z= z) \]

Suppose we know the genders in our sample, finding the parameters is than trivial:
\[ \hat{p} = NumberOfMalesInSample/SampleSize \]
\[ \hat{\mu}_{Male} = AvgMaleHeight \]
\[ \hat{\sigma}_{Male} = SigmaMaleHeight \]


If we see a height and we want to know whether its a Female or Male, we can do it in a probabalistic manner:
\[ p(Z= z|X = x) = p(X = x ,Z= z )/p(X=x)\]
This is called soft clustering.

**But we don't know them..** MLE to the rescue!
Let's define the likelihood of one sample:
\[ p(X = x) = p(Z=Male)* p(X = x|Z=Male) + p(Z=Female)* p(X = x|Z=Fenale)\]
And the complete Data liklihood:
\[\prod_{i}^{n}  p(X = x_i) = \prod_{i}^{n} (p(Z=Male)* p(X = x_i|Z=Male) + p(Z=Female)* p(X = x_i|Z=Fenale))\]
But you will find that this term is impossible to optimize analytically. 
Luckily, there are ways to solve this problem (heuristically):
  
  1. Expectaion maximization algorithm - There exist a lower bound on the log likelihood of the data and it's possible to optimize it iteratively, those improving the likelihood. But we are not guaranteed a global maxima.
  2. Gradient descent (with a a constrain on each sigma being positive semi definite).
  
We will not cover this terms in this course but you will meet the gradient descent algorithm in the future.


Resources : 

* [most detailed explanations](http://cs229.stanford.edu/notes/cs229-notes8.pdf)
* [example in 2d](http://www.ee.bgu.ac.il/~haimp/ml/lectures/lec2/lec2.pdf)
* [video](https://www.youtube.com/watch?v=Rkl30Fr2S38)


Q: 

* What happens if there are more females? 
* What happens if we care more about false positives/false negatives?

A : \

** Wait For Intro to Machine Learning Course! **

### Simple Example
With the binomial distribution

\begin{equation}
L(p)=f_{D}(\mathrm{H}=49 | p)=\left(\begin{array}{c}{80} \\ {49}\end{array}\right) p^{49}(1-p)^{31}\end{equation}
\begin{equation}
0=\frac{\partial}{\partial p}\left(\left(\begin{array}{c}{80} \\ {49}\end{array}\right) p^{49}(1-p)^{31}\right)
\end{equation}
\begin{equation}
0=49 p^{48}(1-p)^{31}-31 p^{49}(1-p)^{30}
\end{equation}
\begin{equation}
=p^{48}(1-p)^{30}[49(1-p)-31 p]
\end{equation}
\begin{equation}
=p^{48}(1-p)^{30}[49-80 p]
\end{equation}

It's clear that the maximum is at p = 49/80.

### Best Pracitces for data Data handling with R

* `mutate()` adds new variables that are functions of existing variables.
* `select()` picks variables based on their names. 
* `filter()` picks cases based on their values.
* `summarize()` reduces multiple values down to a single summary.
* `arrange()` sorts the rows.

```{r, message=FALSE}
library(dplyr) 
```

```{r}
# ?mtcars
mtcars %>% head(2)
```


```{r}
projected_fields <- c('mpg','cyl','disp')
mtcars %>%
  filter(mpg > 2 & cyl > 6) %>%
  select(projected_fields)
```

Additional resources:

+ [dplyr](https://dplyr.tidyverse.org/)
+ [dplyr cheat sheet](https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf)






