---
title: "EX 02 - Data Handling & MLE"
author: "Afek Adler"
date: "`r Sys.Date()`"
output:
  html_document :  
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Today we are going to go over data Handling With R and MLE (Maximum Likelihood Estimation):

   1. Best practices for data handling with R
   1. Motivation example: solving MLE with the expectation maximization algorithm for GMM
   2. Simple examples with the the binomial distribution/poisson/uniform/noraml from wiki

# Best Pracitces for data Data handling with R

R main datatypes:

  * vectors
  * matrices
  * data.frame - matrices with meatadata, added functionallity and allow multiple data types
  * tibbles  - modern take on dataframes 

dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges:

  * `mutate()` adds new variables that are functions of existing variables.
  * `select()` picks variables based on their names. 
  * `filter()` picks cases based on their values.
  * `summarize()` reduces multiple values down to a single summary.
  * `arrange()` sorts the rows.

```{r Impports, message=FALSE}
library(tidyverse)
library(nycflights13)
```


This dataset has 19 columns so the head function is not that usefull when knitting to html.
It is always useful to know how many missing values we have in our dataset, sometimes missing values are not just given to us as NA.
```{r describe dataset}
head(flights,2)
colSums(is.na(flights))/nrow(flights) 
sapply(flights,class)
```
***
**At home - find a better way to print the classes and the % of missing values in R** <br>


## `select()` picks variables based on their names. 

```{r select method}
flight_ditance_airtime <- flights %>% select( distance, air_time) 
flight_ditance_airtime %>% head(2)
```


## `mutate()` adds new variables that are functions of existing variables.
```{r mutate method}
flight_ditance_airtime %>% mutate(mean_speed = distance/air_time) %>% head(2)
```
If you only want to keep the new variables, use `transmute()`:
```{r transmute method}
flight_ditance_airtime %>% transmute(mean_speed = distance/air_time) %>% head(2)
```

##  `filter()` picks cases based on their values.
```{r filter method}
flights %>% filter(is.na(dep_delay)) %>% head(2)
```


##  `arrange()` picks cases based on their values.
```{r arrange  method}
flights %>% arrange(desc(month)) %>% head(2)
```

## `summarize()` reduces multiple values down to a single summary.

```{r summarize  method}
by_month <- group_by(flights,month)
by_month %>% summarise(count = n()) %>% 
  ggplot( mapping = aes(x = month, y = count)) + geom_bar(stat="identity") +  coord_cartesian(ylim = c(2*10^4, 3*10^4))
```

```{r another way}
ggplot(data = flights) + 
  geom_bar(mapping = aes(x = month)) +
 coord_cartesian(ylim = c(2*10^4, 3*10^4))
```


Additional resources:

  + [r4ds](https://r4ds.had.co.nz/transform.html)
  + [dplyr](https://dplyr.tidyverse.org/)
  + [dplyr cheat sheet](https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf)



# 2.MLE 
In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.

If the likelihood function is differentiable, the derivative test for determining maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved explicitly; for instance, the ordinary least squares estimator maximizes the likelihood of the linear regression model. Under most circumstances, however, numerical methods will be necessary to find the maximum of the likelihood function. ("Wikipedia")

## MLE for Human Hight

Assume that we are given data of human height but we don't know their gender.


``` {r}
photo_path <- 'https://ourworldindata.org/uploads/2019/06/distribution-1.png'
destination_path <- 'heights.jpg'
download.file(photo_path,destination_path, mode = 'wb')
```

```{r human height, echo=FALSE, fig.cap="Distribution of height by Gender", out.width = '75%'}
knitr::include_graphics(destination_path)
```

Let's generate a sample:

```{r  sample generation}
set.seed(42)
n_samples = 500

male_avg_h = 178.4
male_std_h = 7.6

female_avg_h = 154  # 164.7 
female_std_h = 7.1

male_h <- rnorm(n_samples, male_avg_h, male_std_h)
female_h <- rnorm(n_samples, female_avg_h, female_std_h)

# a plot using ggplot
all_h <- tibble(h = c(male_h,female_h))

ggplot(all_h, aes(h)) + 
  geom_density(fill = "red", color = "blue") + 
  ggtitle("Human Height [cm]") +
  labs(caption = "n=1000")

# this can also be generated via base R plots, but we highly recommend the use of ggplot2 for producing plots
# Kernel Density Plot
d <- density(all_h$h)

plot(d, main = "Human Height (CM)" )
polygon(d, col="red", border="blue") 
```



Let's assume we know only the heights vector but we don't know the genders at all.
We call the gender a hidden variable because we never observe it.
let's consider a model to generate this vector:

For each sample:

  1. Sample a gender: \[ Z \in \{Female,Male\} \]
NOTE : in our example z is derived from a Bernoulli distribution but the general case is Categorical distribution.
for simplicity lets note: 
 \[ p(Z = Male) = p ,p(Z = Female) = 1-p\]
  2. After we know a gender we need to sample a height - let's assume: 
\[ p(X|Z) = \mathcal{N}\left(\mu_z , \sigma_z^{2}\right)\]

NOTE : it means that there are parameters that describe the height of Males and different parametrs that describe the height Females but their both from normal distribution.

Therefore we can deduce:
\[ p(X = x ,Z= z ) = p(Z=z)* p(X = x|Z= z) \]

Suppose we know the genders in our sample, finding the parameters is than trivial:
\[ \hat{p} = NumberOfMalesInSample/SampleSize \]
\[ \hat{\mu}_{Male} = AvgMaleHeight \]
\[ \hat{\sigma}_{Male} = SigmaMaleHeight \]


If we see a height and we want to know whether its a Female or Male, we can do it in a probabalistic manner, given that we know the parameters of the model:
\[ p(Z= z|X = x) = p(X = x ,Z= z )/p(X=x)\]
This is called soft clustering.
and if we would have to guess a class (gender), than we would guess:
\[ Z_i = argmax_z p(Z= z|X = x_i)\]
This is called Hard clustering.
So if we know the parameters we "know" Z and if we know Z we know the parameters.


**But we don't know them..** MLE to the rescue!
Let's define the likelihood of one sample:
\[ p(X = x) = p(Z=Male)* p(X = x|Z=Male) + p(Z=Female)* p(X = x|Z=Fenale)\]
And the complete Data liklihood:
\[\prod_{i}^{n}  p(X = x_i) = \prod_{i}^{n} (p(Z=Male)* p(X = x_i|Z=Male) + p(Z=Female)* p(X = x_i|Z=Fenale))\]
But you will find that this term is impossible to optimize analytically. 
Luckily, there are ways to solve this problem (heuristically):
  
  1. Expectaion maximization algorithm - There exist a lower bound on the log likelihood of the data and it's possible to optimize it iteratively, those improving the likelihood. But we are not guaranteed a global maxima.
  2. Gradient descent (with a a constrain on each sigma being positive semi definite).
  
We will not cover this terms in this course but you will meet the gradient descent algorithm in the future.


Resources : 

* [most detailed explanations](http://cs229.stanford.edu/notes/cs229-notes8.pdf)
* [example in 2d](http://www.ee.bgu.ac.il/~haimp/ml/lectures/lec2/lec2.pdf)
* [video](https://www.youtube.com/watch?v=Rkl30Fr2S38)


Q: 

* What happens if there are more females? 
* What happens if we care more about false positives/false negatives?

A : \

** Wait For Intro to Machine Learning Course! **

# Simple Example
With the binomial distribution

\begin{equation}
L(p)=f_{D}(\mathrm{H}=49 | p)=\left(\begin{array}{c}{80} \\ {49}\end{array}\right) p^{49}(1-p)^{31}\end{equation}
\begin{equation}
0=\frac{\partial}{\partial p}\left(\left(\begin{array}{c}{80} \\ {49}\end{array}\right) p^{49}(1-p)^{31}\right)
\end{equation}
\begin{equation}
0=49 p^{48}(1-p)^{31}-31 p^{49}(1-p)^{30}
\end{equation}
\begin{equation}
=p^{48}(1-p)^{30}[49(1-p)-31 p]
\end{equation}
\begin{equation}
=p^{48}(1-p)^{30}[49-80 p]
\end{equation}

It's clear that the maximum is at p = 49/80. But let's see how we do it in R using the bulit in [optimize](ehttps://stat.ethz.ch/R-manual/R-devel/library/stats/html/optimize.html) function  

``` {r find MlE}
likelihood <- function(p) p^49*(1-p)^31
tolerance <- 10^(-4) #0.0001
pmax <- optimize(likelihood, c(0, 1), tol = , maximum = T)[[1]]
delta <- abs(pmax- (49/80))
delta
```





