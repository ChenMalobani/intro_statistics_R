---
title: "Point Estimation Methods and Intervals"
subtitle: "Lecture #2"
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: [metropolis, rutgers-fonts]
---

```{css, echo = FALSE}
.remark-slide-content {
  font-size: 28px;
  padding: 20px 80px 20px 80px;
}
.remark-code, .remark-inline-code {
  background: #f0f0f0;
}
.remark-code {
  font-size: 24px;
}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
```


---

# Reminder from previous lecture (1/3)

Last lesson we talked about:

--

   * How data analysis is conducted (import -> tidy -> transform, visualize, model -> communicate)

--

   * About R, very broadly (RStudio, Rmd, scripts, functions, packages)
   
--

   * What a tidy data set looks like (rows = observations, columns = variables, aka features)
   
--

   * Variable types (numeric, date, logical, factor - category, ordinal)
   
--

   * Demonstration of the grammer of graphics (`ggplot2`)
   
--

Then, we also discussed **theory**:

--

   * What are point estimates, i.e.:
   
      * $\theta$ is a *population parameter*
      
      * Estimated by the statistic $\hat{\Theta}$, a *point estimator*
      
      * When it is computed (from a sample), it is called a *point estimate* 
      
---

# Reminder from previous lecture (2/3)

   * Desired properties of point estimates are:
   
      * Unbiased: $E\hat{\Theta} - \theta = 0$
      * Low variance: $V(\hat{\Theta})$ as low as possible

--
   
I've shown that: 

   * The average $\bar{X}=\sum_{i=1}^n{x_i}$ is unbiased: 
      * $E[\bar{X}] = \mu$
      
   * Its variance is $V(\bar{X})=\frac{\sigma^2}{n}$
   
   * In fact, it is the *Minimum Variance Unbiased Estimate* (proof out of scope)
   
--
   
   * We've also seen the bias-variance decomposition, i.e. the Mean Square Error is decomposed into:
      
$$E[(\hat{\Theta}-\theta)^2]=V(\hat{\Theta}) + E(\hat{\Theta} - \theta)^2$$

---

# Reminder from previous lecture (3/3)

We talked about *quantiles*/*percentiles*/*median*:

   * A quantile $q(f)$ of a sample is the **value** for which a specified fraction $f$ of the data values is $\leq q(f)$
   * Note that quartile, quantile, percentile are related/interchangable: 
   
      * 2 quartile = .5 quantile = 50 percentile = median
      
## To sum up

   * The average: $\bar{X}=\sum_{i=1}^n{x_i}$ is an **unbiased** estimator of $E[X]=\mu$
   
   * Sample variance: $S^2=\frac{\sum_{i=1}^n(x_i-\bar{X})^2}{n-1}$ **unbiased** estimator of $V(X)=\sigma^2$
   
      * $V(X)=E(X-E[X])^2=EX^2-(EX)^2$
   
   * Standard deviation: $S$ is a **biased** estimator for $\sigma$ (can't enjoy both worlds)



---

# Why $S^2$ is an unbiased estimator to $\sigma^2$?

$S^2$ is unbiased (you actually saw this in the exercise):

$$ES^2=\frac{1}{n-1}E{\sum_{i=1}^n(X_i-\bar{X})^2}=\frac{1}{n-1}E{\sum_{i=1}^n(x_i^2+\bar{X}^2-2\bar{X}x_i)}$$

--

$$=\frac{1}{n-1}E\left(\sum_{i=1}^nx_i^2-n\bar{X}^2\right)=\frac{1}{n-1}\left[\sum_{i=1}^n{Ex_i^2-nE(\bar{X}^2)}\right]$$

--

Using the fact that $E(x_i^2)=\mu^2+\sigma^2$ and that $E(\bar{X}^2)=\mu^2+\sigma^2/n$ we have:

--

$$E(S^2)=\frac{1}{n-1}\sum_{i=1}^n(\mu^2+\sigma^2)-n(\mu^2+\sigma^2/n)=\sigma^2$$

$$\square$$

---

# Why is $S$ a biased estimator to $\sigma$?

We need to show that $ES\neq\sigma$. Let 

$$S=\sqrt{\sum_{i=1}^n{\frac{(x_i-\bar{X})^2}{n-1}}}$$

--

Consider that $0<Var(S)=E[S^2]-(E[S])^2$ (this is true for any RV, specifically $S$ in this case)

--

Hence

$$(ES)^2<E[S^2] \Leftrightarrow ES<\sqrt{E[S^2]}=\sigma$$ 

$$\square$$

In certain cases, we can directly compute this bias (i.e. what is $ES-\sigma$), for example, if we assume $X\sim N(\mu,\sigma)$, then:

$$\sigma-E(S)\cong\frac{\sigma}{4n}$$ (see [here](https://stats.stackexchange.com/questions/11707/why-is-sample-standard-deviation-a-biased-estimator-of-sigma)).

---

# Estimating $S^2$ - demonstration

Take a normal distribution with
.tiny[
```{r s as a function of n}
set.seed(0)
rv <- tibble(x = rnorm(500, mean = 0, sd = 5)) %>% 
  mutate(cumsum_x = cumsum(x),
         n = seq_along(x),
         average_x = cumsum_x/n,
         sample_var = (1/(n-1))*cumsum(x^2-average_x^2)) %>% 
  slice(-1)

ggplot(rv, aes(x = n, y = sample_var)) + 
  geom_line() + 
  geom_hline(aes(yintercept=25), color = "red") +
  theme_bw()

```
]

---

# The central limit theorem

If $\bar{X}$ is the mean of a random sample of size $n$ taken from a population with mean $\mu$ and finite variance $\sigma^2$, then the limiting form of the distribution of 

$$Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}$$

As $n\rightarrow\infty$ is the standard normal distribution $N(0,1)$

For most purpuses, $n\geq30$ is considered "large enough" (as a rule-of-thumb). 

Lets look at an example, take random variables with $\operatorname{Exp}(\lambda=1)$. What are $\mu$ and $\sigma$?

---

# The distribution of $\bar{X}$ with varying $n$

With $\operatorname{Exp}(\lambda=1)$ we have $\mu=\sigma=1/\lambda=1$

Try increasing `sample_size` and see what happens to the chart.

--

.tiny[
```{r example how a sum of binomials becomes normal}

sample_size <- 1
lambda <- 1

suppressMessages(
rv_binom <- matrix(rexp(n = sample_size*1000, rate = lambda), nrow = sample_size, ncol = 1000) %>% 
  as_tibble(.name_repair = "unique") %>% 
  pivot_longer(cols = everything(), names_to = "var", values_to = "value") %>% 
  group_by(var) %>% 
  summarize(mean = mean(value)) %>% 
  mutate(standardized = (mean - 1/lambda)/((1/lambda)/sqrt(sample_size))))

ggplot(rv_binom, aes(x = standardized)) + 
  geom_histogram(bins = 100) + 
  theme_bw()

```
]

---

# Methods of point estimation

So far we discussed some estimators (i.e., $\bar{X}, S^2$), and the desired properties they hold. But how can we find additional estimators (new statistics)?

We will show three methods:

   * The Bayesian method
   
   * Maximum Liklihood Estimation (MLE)
   
   * Moment method
   
---

# The Bayesian method (reminder: Bayes' rule)

First, a reminder about **Bayes' rule**.

The definition of conditional probability is given by:

--

$$P(A|B)=\frac{P(A\cap B)}{P(B)}$$
For $P(B)>0$. Then, since: $P(A\cap B)=P(B\cap A)$ we also have:

--

$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$

--

If we have a partition $E_1,\ldots, E_k$ (i.e., $\cup E_i=\Omega$ , $E_i\cap E_j=\phi$) we get:

$$P(B) = P(B|E_1)P(E_1)+\ldots+P(B|E_k)P(E_k)$$

--

And finally, we arrive at the general form of Bayes' theorem:

$$P(E_1|B)=\frac{P(B|E_1)P(E_1)}{P(B|E_1)P(E_1)+\ldots+P(B|E_k)P(E_k)}$$

---

# Bayesian Methods of Estimation

We have a parameter $\theta$ for a population with a distribution $f(x|\theta)$. We assume some prior distribution $\pi(\theta)$ (represents our belief on the unknown value of $\theta$).

Assume a sample $x=(x_1,\ldots,x_n)$, then the sampling distribution is $f(x|\theta)$.

We want to use the sampling distribution (obtained from data), and our prior belief in order to yield a *posterior* distribution, i.e., an estimate, of $\theta$.

We use Bayes' rule

$$\pi(\theta|x)=\frac{f(x|\theta)\pi(\theta)}{g(x)}$$

Where:

$$g(x)=\left\{\begin{array}{ll}\sum_\theta f(x|\theta)\pi(\theta) & \theta\text{ discrete}\\\int_{-\infty}^\infty f(x|\theta)\pi(\theta)d\theta & \theta\text{ continuous}\end{array}\right.$$

---

# Bayesian Methods of Estimation - Example

XYZ
