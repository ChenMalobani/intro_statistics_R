---
title: "Inference, Estimation, Intervals, and Hypothesis Testing"
subtitle: "Lecture #2"
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{css, echo = FALSE}
.remark-slide-content {
  font-size: 28px;
  padding: 20px 80px 20px 80px;
}
.remark-code, .remark-inline-code {
  background: #f0f0f0;
}
.remark-code {
  font-size: 24px;
}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3))
library(tidyverse)
```


---

# Reminder from previous lecture (1/3)

Last lesson we talked about:

--

   * How data analysis is conducted (import -> tidy -> transform, visualize, model -> communicate)

--

   * About R, very broadly (RStudio, Rmd, scripts, functions, packages)
   
--

   * What a tidy data set looks like (rows = observations, columns = variables, aka features)
   
--

   * Variable types (numeric, date, logical, factor - category, ordinal)
   
--

   * Demonstration of the grammer of graphics (`ggplot2`)
   
--

Then, we also discussed **theory**:

--

   * What are point estimates, i.e.:
   
      * $\theta$ is a *population parameter*
      
      * Estimated by the statistic $\hat{\Theta}$, a *point estimator*
      
      * When it is computed (from a sample), it is called a *point estimate* 
      
---

# Reminder from previous lecture (2/3)

   * Desired properties of point estimates are:
   
      * Unbiased: $E\hat{\Theta} - \theta = 0$
      * Low variance: $V(\hat{\Theta})$ as low as possible

--
   
I've shown that: 

   * The average $\bar{X}=\sum_{i=1}^n{x_i}$ is unbiased: 
      * $E[\bar{X}] = \mu$
      
   * Its variance is $V(\bar{X})=\frac{\sigma^2}{n}$
   
   * In fact, it is the *Minimum Variance Unbiased Estimate* (proof out of scope)
   
--
   
   * We've also seen the bias-variance decomposition, i.e. the Mean Square Error is decomposed into:
      
$$E[(\hat{\Theta}-\theta)^2]=V(\hat{\Theta}) + E(\hat{\Theta} - \theta)^2$$

---

# Reminder from previous lecture (3/3)

We talked about *quantiles*/*percentiles*/*median*:

   * A quantile $q(f)$ of a sample is the **value** for which a specified fraction $f$ of the data values is $\leq q(f)$
   * Note that quartile, quantile, percentile are related/interchangable: 
   
      * 2 quartile = .5 quantile = 50 percentile = median
      
## To sum up

   * The average: $\bar{X}=\sum_{i=1}^n{x_i}$ is an **unbiased** estimator of $E[X]=\mu$
   
   * Sample variance: $S^2=\frac{\sum_{i=1}^n(x_i-\bar{X})^2}{n-1}$ **unbiased** estimator of $V(X)=\sigma^2$
   
      * $V(X)=E(X-E[X])^2=EX^2-(EX)^2$
   
   * Standard deviation: $S$ is a **biased** estimator for $\sigma$ (can't enjoy both worlds)



---

# Why $S^2$ is an unbiased estimator to $\sigma^2$?

$S^2$ is unbiased (you actually saw this in the exercise):

$$ES^2=\frac{1}{n-1}E{\sum_{i=1}^n(X_i-\bar{X})^2}=\frac{1}{n-1}E{\sum_{i=1}^n(x_i^2+\bar{X}^2-2\bar{X}x_i)}$$

--

$$=\frac{1}{n-1}E\left(\sum_{i=1}^nx_i^2-n\bar{X}^2\right)=\frac{1}{n-1}\left[\sum_{i=1}^n{Ex_i^2-nE(\bar{X}^2)}\right]$$

--

Using the fact that $E(x_i^2)=\mu^2+\sigma^2$ and that $E(\bar{X}^2)=\mu^2+\sigma^2/n$ we have:

--

$$E(S^2)=\frac{1}{n-1}\sum_{i=1}^n(\mu^2+\sigma^2)-n(\mu^2+\sigma^2/n)=\sigma^2$$

$$\square$$

---

# Why is $S$ a biased estimator to $\sigma$?

We need to show that $ES\neq\sigma$. Let 

$$S=\sqrt{\sum_{i=1}^n{\frac{(x_i-\bar{X})^2}{n-1}}}$$

--

Consider that $0<Var(S)=E[S^2]-(E[S])^2$ 

(this is true for any RV, specifically $S$ in this case)

--

Hence

$$(ES)^2<E[S^2] \Leftrightarrow ES<\sqrt{E[S^2]}=\sigma$$ 

$$\square$$

In certain cases, we can directly compute this bias (i.e. what is $ES-\sigma$), for example, if we assume $X\sim N(\mu,\sigma)$, then:

$$\sigma-E(S)\cong\frac{\sigma}{4n}$$ (see [here](https://stats.stackexchange.com/questions/11707/why-is-sample-standard-deviation-a-biased-estimator-of-sigma)).

---

# Estimating $S^2$ demonstration

Take a normal distribution with
.tiny[
```{r s as a function of n}
rv <- tibble(x = rnorm(500, mean = 0, sd = 5)) %>% 
  mutate(cumsum_x = cumsum(x),
         n = seq_along(x),
         average_x = cumsum_x/n,
         sample_var = (1/(n-1))*cumsum(x^2-average_x^2)) %>% 
  slice(-1)

ggplot(rv, aes(x = n, y = sample_var)) + 
  geom_line() + 
  geom_hline(aes(yintercept=25), color = "red") +
  theme_bw()

```
]