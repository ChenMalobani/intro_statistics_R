---
title: "Inference, Estimation, Intervals, and Hypothesis Testing"
subtitle: "Lecture #2"
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```


---

# Reminder from previous lecture (1/3)

Last lesson we talked about:

--

   * How data analysis is conducted (import -> tidy -> transform, visualize, model -> communicate)

--

   * About R, very broadly (RStudio, Rmd, scripts, functions, packages)
   
--

   * What a tidy data set looks like (rows = observations, columns = variables, aka features)
   
--

   * Variable types (numeric, date, logical, factor - category, ordinal)
   
--

   * Demonstration of the grammer of graphics (`ggplot2`)
   
--

Then, we also discussed **theory**:

--

   * What are point estimates, i.e.:
   
      * $\theta$ is a *population parameter*
      
      * Estimated by the statistic $\hat{\Theta}$, a *point estimator*
      
      * When it is computed (from a sample), it is called a *point estimate* 
      
---

# Reminder from previous lecture (2/3)

   * Desired properties of point estimates are:
   
      * Unbiased: $E\hat{\Theta} - \theta = 0$
      * Low variance: $V(\hat{\Theta})$ as low as possible

--
   
I've shown that: 

   * The average $\bar{X}=\sum_{i=1}^n{x_i}$ is unbiased: 
      * $E[\bar{X}] = \mu$
      
   * Its variance is $V(\bar{X})=\frac{\sigma^2}{n}$
   
   * In fact, it is the *Minimum Variance Unbiased Estimate* (proof out of scope)
   
--
   
   * We've also seen the bias-variance decomposition, i.e. the Mean Square Error is decomposed into:
      
$$E[(\hat{\Theta}-\theta)^2]=V(\hat{\Theta}) + E(\hat{\Theta} - \theta)^2$$

---

# Reminder from previous lecture (3/3)

We talked about *quantiles*/*percentiles*/*median*:

   * A quantile $q(f)$ of a sample is the **value** for which a specified fraction $f$ of the data values is $\leq q(f)$
   * Note that quartile, quantile, percentile are related/interchangable: 
   
      * 2 quartile = .5 quantile = 50 percentile = median
      
## To sum up

   * The average: $\bar{X}=\sum_{i=1}^n{x_i}$ is an **unbiased** estimator of $E[X]=\mu$
   
   * Sample variance: $S^2=\frac{\sum_{i=1}^n(x_i-\bar{X})^2}{n-1}$ **unbiased** estimator of $V(X)=\sigma^2$
   
      * $V(X)=E(X-E[X])^2=EX^2-(EX)^2$
   
   * Standard deviation: $S$ is a **biased** estimator for $\sigma$ (can't enjoy both worlds)



---

# Why $S^2$ is an unbiased estimator to $\sigma^2$?

$S^2$ is unbiased: 

$$ES^2=\frac{1}{n-1}E{\sum_{i=1}^n(X_i-\bar{X})^2}=\frac{1}{n-1}E{\sum_{i=1}^n(x_i^2+\bar{X}^2-2\bar{X}x_i)}$$

--

$$=\frac{1}{n-1}E\left(\sum_{i=1}^nx_i^2-n\bar{X}^2\right)=\frac{1}{n-1}\left[\sum_{i=1}^n{Ex_i^2-nE(\bar{X}^2)}\right]$$

--

Using the fact that $E(x_i^2)=\mu^2+\sigma^2$ and that $E(\bar{X}^2)=\mu^2+\sigma^2/n$ we have:

--

$$E(S^2)=\frac{1}{n-1}\sum_{i=1}^n(\mu^2+\sigma^2)-n(\mu^2+\sigma^2/n)=\sigma^2$$

$$\square$$

---

# Why is $S$ a biased estimator to $\sigma$?

We need to show that $ES\neq\sigma$. Let 

$$S=\sqrt{\sum_{i=1}^n{\frac{(x_i-\bar{X})^2}{n-1}}}$$

--

Consider that $0<Var(S)=E[S^2]-(E[S])^2$ 

(this is true for any RV, specifically $S$ in this case)

--

Hence

$$(ES)^2<E[S^2] \Leftrightarrow ES<\sqrt{E[S^2]}=\sigma$$ 

$$\square$$