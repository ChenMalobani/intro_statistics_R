---
title: "Statistical inference for Two Samples"
subtitle: "Lecture #5"
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: [metropolis, rutgers-fonts]
---

```{css, echo = FALSE}

.remark-code {
  font-size: 24px;
}

.huge { 
  font-size: 200%;
}
.tiny .remark-code {
  font-size: 50%;
}

.small .remark-code{
   font-size: 85% !important;
}

.small {
   font-size: 85% !important;
}

.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}

table { display: inline-block; }

th, td {
   padding: 5px;
}

.small-slide {
   font-size: 70% !important;
}

.image-50 img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}

.right-plot {
   width: 60%;
   float: right;
   padding-left: 1%;
   bottom: 0px;
   right: 0px;
   position: absolute;
}



```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
library(countdown)
```

# Reminder from previous lecture

   * Hypothesis tests (e.g., of proportion in the attractive flu shot experiment)
   
   * Type-I and Type-II errors $\alpha=P(H_1|H_0), \beta=P(H_0|H_1)$
   
   * General framework for hypothesis testing (the 8 steps)
   
      * Parameter -> Null hypothesis -> Altermative -> Significance -> Statistic -> Rejection criteria
      * Sample, computation -> Decision
   
   * P-value and the relationship to confidence intervals
   
   * Designing the sample size for a desired power $1-\beta$
   
   * Goodness of fit hypothesis test (comparing to distribution)
   
   * Words of caution about HARKing and multiple comparisons
   
---

# Are men and women different? ♂  ♀

Yes. Of course there are gender differences, but this is a great example to start today's lecture with!

--

Here is a research published in PLOS One, about gender differences relating to **empathy and moral cognition**:

   * Baez, Sandra, et al. "Men, women… who cares? A population-based study on sex differences and gender roles in empathy and moral cognition." *PloS one* 12.6 (2017): e0179336.
      * [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0179336](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0179336)

--

   * The research uses statistical means to compare measures such as the **Interpersonal Reactivity Index** (IRI) to see if women score higher than men, in a **statistically significant manner**.
      * For example, figure 3. (significant sex differences in self-reported empathy) shows that women **percive** themselves as being more interpersonal than what men perceive. However, these might be driven by stereotypes, and might be apparant only in self-assesment instruments.

--
      
   * We won't go over the entire study, but I highly recommend reading it: based on a very large sample $n=10802$ and refutes some common stereotypes. (Also a misleading presentation e.g. y-axis inconsistencies, which we can learn from).
  
--

   * One very cool feature of this study is that the authors made the data available for the public [here](https://figshare.com/s/06fa32a3c57b93adc1a7).
   
---

# What is it to us?

In previous lectures we saw confidence intervals and hypothesis tests relating to a **single parameter** (single sample)

--

Today we will explore the comparison between **two parameters** (two samples, e.g., male-female, tall-short, before-after).

--

In the gender empathy example, let $\mu_{\text{f}}, \mu_\text{m}$ represent female and male empathy scores. We want to test the following hypothesis:

   * $H_0: \mu_\text{f}=\mu_\text{m}$
   * $H_1: \mu_\text{f}>\mu_\text{m}$
   
--
   
Then, set $\mu_\text{diff} = \mu_\text{f}-\mu_\text{m}$ and

   * $H_0: \mu_\text{diff}=0$
   * $H_1: \mu_\text{diff}>0$

--

This, more-or-less, brings us back to what we already learned last week.

--

Most of today's material is covered in Montgomery, chapter 10.

---

# Difference in means - variance known

Assume

   * $X_{11},\ldots,X_{1n_1}$ is a random sample from population 1.
   * $X_{21},\ldots,X_{2n_n}$ is a random sample from population 2.
   * The two populations $X_1$ and $X_2$ are independent.
   * Both populations are normally distributed.
   
Then, what is $E[\bar{X}_1-\bar{X}_2]$, and $\operatorname{Var}[\bar{X}_1-\bar{X}_2]$?

--

$$E[\bar{X}_1-\bar{X}_2] = \mu_1-\mu_2$$

$$\operatorname{Var}[\bar{X}_1-\bar{X}_2]=\operatorname{Var}(\bar{X}_1)-\operatorname{Var}(\bar{X}_2)=\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}$$ 

Using these, and the fact that the sum of two normally distributed variables is also normal we obtain...

---

# Difference in means - variance known (2)

The following quantity is normally distributed $\mathcal{N}(0,1)$:

$$Z = \frac{\bar{X}_1 - \bar{X}_2 - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma^2_2}{n_2}}}$$

--

Null hypothesis: $H_0: \mu_1-\mu_2=\Delta_0$

Alternative Hypotheses (rejection criterion):

   * $H_1: \mu_1-\mu_2\neq\Delta_0$ $\quad(z_0>z_{\alpha/2} \text{ or } z_0<-z_{\alpha/2})$
   
   * $H_1: \mu_1-\mu_2>\Delta_0$ $\quad(z_0>z_\alpha)$
   
   * $H_1: \mu_1-\mu_2>\Delta_0$ $\quad(z_0<-z_\alpha)$

---

# Choice of sample size

Similarly to what we've seen in the last lecture, given $\alpha, \beta, \Delta$, we can determin the desired sample sizes $n_1, n_2$, by writing (example for a two sided hypothesis):

$$\beta = P(H_0|H_1)=P\left(\left. z_{\alpha/2}<\frac{\bar{X}_1-\bar{X}_2-\Delta_0}{\sqrt{\frac{\sigma_1^2}{{n_1}} + \frac{\sigma_2^2}{{n_2}}}}< z_{1-\alpha/2}\right| \mu_1-\mu_2=\Delta_0+\delta\right)$$

--

Define $\Delta = \Delta_0 + \delta = \mu_1-\mu_2$, if we play a bit with the expression in the parentheses, and using $\Phi^{-1}(\cdot)$, we can reach the following expression:

$$n\approx\frac{(z_{\alpha/2}+z_\beta)^2(\sigma_1^2+\sigma_2^2)}{(\Delta-\Delta_0)^2}$$

where $n=n_1=n_2$ and the total sample size is $2n$.

--


For a one sided alternative hypothesis, the equivalent formulat would be

$$n=\frac{(z_{\alpha}+z_\beta)^2(\sigma_1^2+\sigma_2^2)}{(\Delta-\Delta_0)^2}$$

---

# Confidence intervals on a difference in means

When we discussed confidence intervals two weeks ago, we left out confidence intervals for the difference between two means, but this is actually almost the same (now that you've seen $Z$ for the difference). I.e.:

$$P\left[z_{\alpha/2}\leq\frac{\bar{X}_1-\bar{X}_2-(\mu_1-\mu_2)}{\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}}\leq z_{1-\alpha/2}\right]=1-\alpha$$

Hence, a confidence interval for $\mu_1-\mu_2$ can be obtained by using:

$$\bar{x}_1-\bar{x}_2+z_{\alpha/2}\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}\leq \mu_1-\mu_2\leq \bar{x}_1-\bar{x}_2+z_{1-\alpha/2}\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}$$

--

Or in the one-sided case:

$$\mu_1-\mu_2 \leq \bar{x}_1-\bar{x}_2+z_{1-\alpha}\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}$$ 

--

(Or the other side)
   
$$\mu_1-\mu_2 \geq \bar{x}_1-\bar{x}_2+z_{\alpha}\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}$$ 

---

# Your turn (class exercise) - do mobile phones impact our health?

In **pairs**, try to devise an experiment plan, that would test whether **mobile phones impact our health**.

In your answer relate to the following points:

   * What do you consider as an "effect"? (i.e., what kind of health measure?)

   * How do you select and/or separate the groups participating in your experiment?
   
   * How do you neutralize other factors which might intervene with the experiment? (like selection of participants or other factors)
   
   * What would you use as a statistcal measure?
   
   * What sample size would you need?
   
   * What would be the hypotheses 8-step procedure of the experiment? reminder:
      * Parameter -> Null hypothesis -> Altermative -> Significance -> Statistic -> Rejection criteria
      * Sample, computation -> Decision

```{r countdown for experiment designing, echo=FALSE}
countdown::countdown(minutes = 10)
```

---

# Difference in means - variance unknown

Let's assume that $\sigma$ is unknown, but that $\sigma_1=\sigma_2=\sigma$.

We've seen that $S_i^2 = \frac{\sum_j(X_{ij}-\bar{X}_i)^2}{n_i-1}$ is an unbiased estimator for $\sigma_i^2$.

Introducing the pooled estimator of $\sigma^2$, denoted by $S^2_p$:

$$S_p^2=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}$$

--

This is a weighted average of $S_1^2$ and $S_2^2$. 

--

Each $S_i$ contributes $n_1-1$ degrees of freedom so overall $S_p^2$ has $n_1+n_2-2$ degrees of freedom.

Given normality assumptions (or $n_1$ and $n_2$ large enough), we can use the following statistic as a student's t distribution with $n_1+n_2-2$ degrees of freedom:

$$T = \frac{\bar{X}_1-\bar{X}_2-(\mu_1-\mu_2)}{S_p\sqrt{1/n_1+1/n_2}}$$

---

# Paired or unpaired t-test?

---

# Effect size and Cohen's d

---

# Two sample Confidence intervals 

---

# Variance of two proportions (F-test)

---

# Going back for some more "goodness of fit"

We are going to jump back to something we talked about in the previous lecture - the goodness of fit $\chi^2$ test. This time however we will use it to test if two distributions are independent.