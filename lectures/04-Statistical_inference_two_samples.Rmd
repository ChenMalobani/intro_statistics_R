---
title: "Statistical inference for Two Samples"
subtitle: "Lecture #5"
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: [metropolis, rutgers-fonts]
---

```{css, echo = FALSE}

.remark-code {
  font-size: 24px;
}

.huge { 
  font-size: 200%;
}
.tiny .remark-code {
  font-size: 50%;
}

.small .remark-code{
   font-size: 85% !important;
}

.small {
   font-size: 85% !important;
}

.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}

table { display: inline-block; }

th, td {
   padding: 5px;
}

.small-slide {
   font-size: 70% !important;
}

.image-50 img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}

.right-plot {
   width: 60%;
   float: right;
   padding-left: 1%;
   bottom: 0px;
   right: 0px;
   position: absolute;
}



```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
library(countdown)
```

# Reminder from previous lecture

   * Hypothesis tests (e.g., of proportion in the attractive flu shot experiment)
   
   * Type-I and Type-II errors $\alpha=P(H_1|H_0), \beta=P(H_0|H_1)$
   
   * General framework for hypothesis testing (the 8 steps)
   
      * Parameter -> Null hypothesis -> Altermative -> Significance -> Statistic -> Rejection criteria
      * Sample, computation -> Decision
   
   * P-value and the relationship to confidence intervals
   
   * Designing the sample size for a desired power $1-\beta$
   
   * Goodness of fit hypothesis test (comparing to distribution)
   
   * Words of caution about HARKing and multiple comparisons
   
---

# Are men and women different? ♂  ♀

Yes. Of course there are gender differences, but this is a great example to start today's lecture with!

--

Here is a research published in PLOS One, about gender differences relating to **empathy and moral cognition**:

   * Baez, Sandra, et al. "Men, women… who cares? A population-based study on sex differences and gender roles in empathy and moral cognition." *PloS one* 12.6 (2017): e0179336.
      * [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0179336](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0179336)

--

   * The research uses statistical means to compare measures such as the **Interpersonal Reactivity Index** (IRI) to see if women score higher than men, in a **statistically significant manner**.
      * For example, figure 3. (significant sex differences in self-reported empathy) shows that women **percive** themselves as being more interpersonal than what men perceive. However, these might be driven by stereotypes, and might be apparant only in self-assesment instruments.

--
      
   * We won't go over the entire study, but I highly recommend reading it: based on a very large sample $n=10802$ and refutes some common stereotypes. (Also a misleading presentation e.g. y-axis inconsistencies, which we can learn from).
  
--

   * One very cool feature of this study is that the authors made the data available for the public [here](https://figshare.com/s/06fa32a3c57b93adc1a7).
   
---

# What is it to us?

In previous lectures we saw confidence intervals and hypothesis tests relating to a **single parameter** (single sample)

--

Today we will explore the comparison between **two parameters** (two samples, e.g., male-female, tall-short, before-after).

--

In the gender empathy example, let $\mu_{\text{f}}, \mu_\text{m}$ represent female and male empathy scores. We want to test the following hypothesis:

   * $H_0: \mu_\text{f}=\mu_\text{m}$
   * $H_1: \mu_\text{f}>\mu_\text{m}$
   
--
   
Then, set $\mu_\text{diff} = \mu_\text{f}-\mu_\text{m}$ and

   * $H_0: \mu_\text{diff}=0$
   * $H_1: \mu_\text{diff}>0$

--

This, more-or-less, brings us back to what we already learned last week.

--

Most of today's material is covered in Montgomery, chapter 10.

---

# Difference in means - variance known

Assume

   * $X_{11},\ldots,X_{1n_1}$ is a random sample from population 1.
   * $X_{21},\ldots,X_{2n_n}$ is a random sample from population 2.
   * The two populations $X_1$ and $X_2$ are independent.
   * Both populations are normally distributed.
   
Then, what is $E[\bar{X}_1-\bar{X}_2]$, and $\operatorname{Var}[\bar{X}_1-\bar{X}_2]$?

--

$$E[\bar{X}_1-\bar{X}_2] = \mu_1-\mu_2$$

$$\operatorname{Var}[\bar{X}_1-\bar{X}_2]=\operatorname{Var}(\bar{X}_1)-\operatorname{Var}(\bar{X}_2)=\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}$$ 

Using these, and the fact that the sum of two normally distributed variables is also normal we obtain...

---

# Difference in means - variance known (2)

The following quantity is normally distributed $\mathcal{N}(0,1)$:

$$Z = \frac{\bar{X}_1 - \bar{X}_2 - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma^2_2}{n_2}}}$$


# Difference in means - variance unknown

---

# Choice of sample size

---

# Paired or unpaired t-test?

---

# Effect size and Cohen's d

---

# Two sample Confidence intervals 

---

# Variance of two proportions (F-test)

---

# Going back for some more "goodness of fit"

We are going to jump back to something we talked about in the previous lecture - the goodness of fit $\chi^2$ test. This time however we will use it to test if two distributions are independent.