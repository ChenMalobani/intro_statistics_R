---
title: "Simple Linear Regression and Correlation"
subtitle: "Lecture #6"
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: [metropolis, rutgers-fonts]
---

```{css, echo = FALSE}

.remark-code {
  font-size: 24px;
}

.huge { 
  font-size: 200%;
}
.tiny .remark-code {
  font-size: 50%;
}

.small .remark-code{
   font-size: 85% !important;
}

.small {
   font-size: 85% !important;
}

.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}

table { display: inline-block; }

th, td {
   padding: 5px;
}

.small-slide {
   font-size: 70% !important;
}

.image-50 img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}

.right-plot {
   width: 60%;
   float: right;
   padding-left: 1%;
   bottom: 0px;
   right: 0px;
   position: absolute;
}



```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
library(countdown)
```

# Reminder from previous lecture

We talked about two sample statistics
   
   * Hypothesis testing of means, e.g.: $H_0: \mu_1=\mu_2 \text{ vs } H_1: \mu_1\neq\mu_2$
      
   * Confidence intervals, e.g.: $\mu_1-\mu_2\in \bar{x}_1-\bar{x}_2 + [z_{\alpha/2},z_{1-\alpha/2}]\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}$
   
   * Power computations desired sample size with two sample statistics
   
   * Paired versus unpaired tests
   
   * Comparing variance using the $F$ test
   
   * Goodness of fit for examining variable independence, i.e., using $\chi^2$ test
   
   * We saw some examples (men vs. women, mobile phone and health)
   
---

# Inference on two population proportions

We consider the case of two binomial parameters $p_1, p_2$. Let $X_1, X_2$ represent the number of successes in each sample. $\hat{P}_i=X_i/n_i$, have approximately normal distributions.

$$Z=\frac{\hat{P}_1-\hat{P}_2-(p_1-p_2)}{\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}}$$
Is distributed approximately as $Z\sim\mathcal{N}(0,1)$.

--

Under the null hypothesis $H_0: p_1=p_2=p$ we have:

$$Z = \frac{\hat{P}_1-\hat{P}_2}{\sqrt{p(1-p)(1/n_1 + 1/n_2)}}$$

--

Where an estimator to $p$ is given by:

$$\hat{P}=\frac{X_1+X_2}{n_1+n_2}$$

---

# The test procedure for comparing two population proportions

Null hypothesis: $H_0: p_1=p_2$

Test statistic: $Z_0=\frac{\hat{P}_1-\hat{P}_2}{\sqrt{\hat{P}(1-\hat{P})(1/n_1 + 1/n_2)}}$

Alternative hypothesis (rejection criteria):

   * $H_1:p_1\neq p_2 \quad(z_0>z_{1-\alpha/2} \text{ or } z_0<z_{\alpha/2})$
   
   * $H_1:p_1>p_2 \quad (z_0>z_{1-\alpha})$
   
   * $H_1:p_1<p_2 \quad (z_0<z_{\alpha})$
   
---

# Setting the sample sizes when comparing two population proportions

Very similar to what we've shown in the last lecture for one sample, but with a slightly different computation for the standard deviation under $H_1$. For example, in the two sided case we have:

$$\beta=\Phi\left[\frac{z_{1-\alpha/2}\sqrt{\bar{p}\bar{q}(1/n_1+1/n_2)}-(p_1-p_2)}{\sigma_{\hat{P}_1-\hat{P}_2}}\right]-\Phi\left[\frac{z_{\alpha/2}\sqrt{\bar{p}\bar{q}(1/n_1+1/n_2)}-(p_1-p_2)}{\sigma_{\hat{P}_1-\hat{P}_2}}\right]$$

--

With $\bar{p}=\frac{n_1p_1+n_2p_2}{n_1+n_2}, \bar{q}=\frac{n_1(1-p_1)+n_2(1-p_2)}{n_1+n_2}$ and

$$\sigma_{\hat{P}_1-\hat{P}_2}=\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}$$

--

We can obtain the suggested sample (or power) using `pwr::pwr.2p.test` or `pwr::pwr2p2n.test`.

.small[
```{r demonstration for sample size using pwr}
pwr::pwr.2p2n.test(h = pwr::ES.h(p1 = 0.2, p2 = 0.3),
                   n1 = 150, n2 = NULL,
                   sig.level = 0.05,
                   power = 0.8,
                   alternative = "less")
```
]

---

# Effect Size

We discussed p-value as the extent to which a statistical finding is significant. However, it is not the sole measure for the strength of a statistical finding.

--

In this context, see the ASA statement on $p$-Values [here](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)

--

**Effect size** measures the magnitude of a phenomena. Effect size is a generic name for various measures such as:

   * $R^2$ in linear regression
   
   * $\rho$ Pearson correlation coefficient between two variables
   
   * Cohen's $d$ which relates to the difference between means (which we will now discuss)
   
   * Many more

---

# Effect Size - Cohen's $d$

The difference between two means divided by standard deviation, i.e.:

$$d=\frac{\bar{X}_1-\bar{X}_2}{S_p}$$

Where $S_p$ is the pooled standard deviation:

$$S_p=\sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}}$$
.small[
```{r example for cohens d}
effsize::cohen.d(formula = best3squat_kg ~ sex, data = ipf)
```
]

---

# Simple Linear Regression: motivation

---

# The Basic Regression Model: description and assumptions

---

# Properties of the least squares estimators

---

# Hypothesis tests in simple linear regression

---

# Confidence intervals

---

# Predicting new observations

---

# Adequacy of the linear regression model

---

# Transformations

---

# Correlation