---
title: "Multiple Linear Regression and Correlation"
subtitle: "Lecture #7"
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: [metropolis, rutgers-fonts]
---

```{css, echo = FALSE}

.remark-code {
  font-size: 24px;
}

.huge { 
  font-size: 200%;
}
.tiny .remark-code {
  font-size: 50%;
}

.small .remark-code{
   font-size: 85% !important;
}

.small {
   font-size: 85% !important;
}

.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}

table { display: inline-block; }

th, td {
   padding: 5px;
}

.small-slide {
   font-size: 70% !important;
}

.image-50 img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}

.right-plot {
   width: 60%;
   float: right;
   padding-left: 1%;
   bottom: 0px;
   right: 0px;
   position: absolute;
}



```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
library(countdown)
```

# Reminder from previous lecture

We focused on simple linear regression.

   * We saw how simple linear regression can be used to find the relationship between two variables
   
   * For example, flight height and the number of bird strikes (or `log` or bird strikes)
   
--
   
We discussed the base assumptions in linear regression:

   * Linearity $Y=\beta_0+\beta_1x + \epsilon$
   
   * $\epsilon \sim \mathcal{N}(0,\sigma_\epsilon)$
   
   * For hypothesis testing on $\beta$ we also require homoscedastity
   
--

We discussed the objective function: the least squares $L$

--

We have shown how to find $\beta_0$ and $\beta_1$ from the partial derivative of $\partial L/\partial \beta_i$
   
---

# Reminder from previous lecture (2)

$$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}, \quad \hat{\beta}_1=\frac{S_{xy}}{S_{xx}}$$

Where:

$$S_{xx}=\sum_{i=1}^n{(x_i-\bar{x})^2}, \quad S_{xy}=\sum_{i=1}^n{(y_i-\bar{y})(x_i-\bar{x})}$$

--

We discussed the importance of $SS_E = \sum_{i=1}^n(y_i-\hat{y})^2$, and talked about its role in estimating $\sigma_\epsilon$:

$$\hat{\sigma}_\epsilon^2=\frac{SS_E}{(n-2)}$$

--

The variance of the coefficients is given by:

$$\operatorname{Var}(\hat{\beta}_1)= \frac{\sigma_\epsilon^2}{S_{xx}}$$

$$\operatorname{Var}({\hat{\beta_0}}) = \sigma_\epsilon^2\left[\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right]$$

---

# Reminder from previous lecture (3)

The variance of $\beta_0, \beta_1$ helped us devise a statistic and a hypothesis test for the parameters, i.e.:

$$T_0=\frac{\hat{\beta}_1 - 0}{\sqrt{\hat{\sigma}_\epsilon^2/S_{xx}}}$$

--

We've also seen the decomposition of the overall variance to the regression variance and error variance, i.e.:

$$\sum_{i=1}^n{(y_i-\bar{y})^2} = \sum_{i=1}^n{(\hat{y}_i-\bar{y})^2} + \sum_{i=1}^n{(y_i-\hat{y})^2}$$

--

$$SS_T=SS_R + SS_E$$

Where $SS_R$ has 1 degree of freedome, $SS_E$ has $n-2$ degrees of freedom, and $SS_T$ has $n-1$ degrees of freedom.

--

Under a null hypothesis of $H_0: \beta_1=0$, both $SS_R, SS_E$  are $\chi^2$ distributed, with $1, n-2$ degrees of freedom respectively.

--

This led us to an additional test, using analysis of variance.

---

# Analysis of Variance for Regression Significance

Then, the following statistic would be F-distributed, under the null hypothesis:

$$F_0=\frac{SS_R/1}{SS_E/(n-2)} = \frac{MS_R}{MS_E}$$
--

The intuition behind the statistic is: 

   * As the mean square error $MS_E$ decreases; and
--

   * The variance explained by the regression model $MS_R$ increases
--

   * The model is a good fit to the data 
--

   * Hence, the null hypothesis of no model, i.e., $\beta_1=0$, is rejected

--

## ANOVA (Analysis of Variance) Table

.small[
| Source of Variation | Sum of Squares | df | Mean Squares | $F_0$ |
|----------------------|-----------------|-------|:------------:|---------------------|
| Regression | $SS_R$ | $1$ | $MS_R$ | $\frac{MS_R}{MS_E}$ |
| Error | $SS_E$ | $n-2$ | $MS_E$ |  |
| Total | $SS_T$ | $n-1$ |  |  |
]

---

# Coefficient of determination $R^2$

We would like to measure the effect size of the regression. One possibility to measure the effect size is to use $R^2$:

$$R^2=\frac{SS_R}{SS_T}=1-\frac{SS_E}{SS_T}$$

Since $SS_T=SS_R + SS_E$, and all sizes are non negative:

$$0\leq R^2 \leq1$$

As the fit is better, $R^2$ increases.

---

# Correlation

In probability, the correlation coefficient between two variables $X$ and $Y$ is defined as:

$$\rho=\frac{\sigma_{XY}}{\sigma_X\sigma_Y}$$

Where $\sigma_{XY}^2=\operatorname{Cov}(X,Y)=E[(X-\mu_x)(Y-\mu_Y)]=E[XY]-E[X]E[Y]$

The correlation $\rho\in[-1, 1]$. When $\rho=1 \text{ or } -1$, this means that the two variables have a linear relationship between them, and if it is 0 then the covariance is 0 and the two variables are independent.

We would like to see the relationship between $\rho$ and $R^2$.

--

We can estimate $\rho$ using:

$$\hat{\rho} = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}$$


---

# Correlation (2)

We have defined $SS_T = \sum(y_i-\bar{y})^2=S_{yy}$

--

Remember that:

   * $\hat{\beta}_0 = \bar{y}-\hat{\beta}_1\bar{x}$
   
   * $\hat{\beta}_1 = \frac{S_{xy}}{S_{xx}} = \hat{\rho}\sqrt{\frac{S_{yy}}{S_{xx}}}$, which demonstrates the relationship between $\hat{\rho}$ and $\beta_1$. 

--

Also note that: 

$$SS_E=\sum(y_i-\hat{y}_i)^2=S_{yy}-\beta_1S_{xy}$$

To see this use the above formulas for $\beta_0$ and $\beta_1$:

$$SS_E=\sum(y_i-\hat{y}_i)^2=\sum(y_i-\hat\beta_0-\hat\beta_1x_i)^2=\sum\left((y_i-\bar y)-\beta_1(x_i-\bar x)\right)^2=\\ S_{yy}-2\beta_1S_{xy}+\beta_1^2S_{xx} = S_{yy}-\beta_1S_{xy}$$

$$1-R^2 = 1-SS_R/SS_T = SS_E/S_{yy} = SS_E/SS_T = 1-\beta_1S_{xy}/S_{yy} = 1-\frac{S_{xy}^2}{S_{xx}S_{yy}} = 1-\hat\rho^2$$

---

# Correlation (3)
.small[
Hence, we have proven that coefficient of determination $R^2$ is in fact the estimate for the square of the correlation coefficient $\hat\rho^2$ of $X$ and $Y$. 
]

--

.small[
In general, correlation is interesting because it can help us find simple association rules between variables. Let's see an example with movies genres. Adopted from a [scraped imdb](https://github.com/sundeepblue/movie_rating_prediction) source.
]

--

.tiny[
```{r load genres compute cor}
library(tidyverse)
# "https://raw.githubusercontent.com/sarid-ins/statistical_learning_course/master/datasets/scraped_imdb/movie_db_clean.csv")
movies <- suppressWarnings(read_csv("data/movie_db_clean.csv", col_types = cols()) %>% 
  janitor::clean_names() %>% select(adventure:western) %>%  mutate_all(~.*1))
genres_corr <- cor(movies)
#ggcorrplot::ggcorrplot(genres_corr, type = "upper", hc.order = T, hc.method = "complete")
```
]
.right-plot[
```{r do the genres plot, echo=FALSE, fig.dim=c(6,6)}
ggcorrplot::ggcorrplot(genres_corr, type = "upper", hc.order = T, hc.method = "complete", tl.cex = 9)
   
```
]
---

# Multiple Linear Regression - Background 

.small[So far we treated regression with only two variable (one dependent, $Y$ and one independent $X$).]

--

.small[
In most cases, we will have more than one independent variable, i.e., $X_1,\ldots,X_p$. Our model becomes:

$$Y=\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_pX_p + \epsilon$$
]

--

.small[
This can also be extended to accomodate for more complex relationships such as:

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_{12}X_{1}X_2 + \epsilon$$
]

--

.small[
or

$$Y = \sin(X_1) + \cos(X_1X_2) + \epsilon$$
]

--

.small[
The principles and assumptions we discussed still hold, but some careful handling is needed, and this is what we will discuss today.
]

.tiny[
```{r some complex relationship}
ex1 <- crossing(x1 = seq(0, 5, 0.05),
         x2 = seq(0, 5, 0.05)) %>% 
  mutate(y = sin(x1) + cos(x2*x1) + rnorm(NROW(x1), sd = 0.05))
p1 <- ggplot(ex1, aes(x = x1, y = x2, z = y)) + 
  geom_contour() + 
  theme_bw()
p2 <- ggplot(ex1, aes(x = x1, y = x2, fill = y)) + 
  geom_tile() + 
  theme_bw()
#cowplot::plot_grid(p1, p2)
```
]

.right-plot[
```{r some complex relationship the plot, fig.dim=c(8,3), echo = FALSE}
cowplot::plot_grid(p1, p2)
```
]

---

# Motivation

Let's analyze the example from last lecture (planes and birds), only this time, enrich the problem.

--

.tiny[
```{r}
wildlife_medium <- read_csv("data/wildlife_impacts_medium.csv", col_types = cols())
lm(formula = log(n) ~ ., data = wildlife_medium) %>% 
  summary()
```
]

--

Can you say what's the problem of examining this dataset, in this context? (think about possible biases)

---

# Least Squares Estimation of the Parameters

We will use matrix notation.  For each observation $i$ we have:

$$y_i = \beta_0 + \beta_1x_{i1} + \ldots + \beta_kx_{ik} + \epsilon_i, \quad i = 1,\ldots,n$$

This can be represented as:

$$y = X\beta+\epsilon$$

Where:

$$y = \left[\begin{array}{c}y_1\\\vdots\\y_n\end{array}\right], \quad X=\left[\begin{array}{cccc}1 & x_{11} & \ldots & x_{1k}\\\vdots & \vdots & \ddots & \vdots\\1 & x_{n1} & \ldots & x_{nk}\end{array}\right], \quad \beta = \left[\begin{array}{c}\beta_0\\\vdots\\\beta_k\end{array}\right],\quad \epsilon=\left[\begin{array}{c}\epsilon_1\\\vdots\\\epsilon_n\end{array}\right]$$

--

We are looking for $\beta$ which minimizes $L = \epsilon^t\epsilon=(y-X\beta)(y-X\beta)$

$$\frac{\partial L}{\partial\beta}=0$$

---

# Least Squares Estimation of the Parameters (2)

The resulting equations are given by:

$$X^tX\hat\beta=X^ty$$

--

In case that $X^tX$ is a non-singular matrix (i.e., invertible), the solution is unique and equals

$$\hat\beta = (X^tX)^{-1}X^ty$$

--

Once $\hat{\beta}$ is found, we can use it to predict our values:

$$\hat{y} = X\hat{\beta}$$

--

We can also compute the residuals:

$$e = y-\hat{y}$$

--

Let $p=k+1$ (the number of parameters including the constant $\beta_0$), then:

$$\hat\sigma^2=\frac{\sum{e_i^2}}{n-p}=\frac{SS_E}{n-p}$$

Is an unbiased estimate of $\sigma_\epsilon^2$

---

# Hypothesis Tests

The vector $\hat\beta$ is an unbiased estimate:

$$E\hat\beta = E[(X^tX)^{-1}X^ty]=E[(X^tX)^{-1}X^t(X\beta+\epsilon)]=E[I\beta + (X^tX)^{-1}X^t\epsilon] = \beta$$

--

The $\beta$ coefficients' variance is given by diagonal elements of $(X^tX)^{-1}$ times $\sigma^2$.

--

Now that we have found the expected value and the variance, we are ready for some hypothesis tests.

--

We are going to use the following set of hypothesis:

   * $H_0:\beta_1=\beta_2=\ldots=\beta_k=0$
   
   * $H_1: \exists i \text{ such that } \beta_i\neq0$

--

Under the null hypothesis, $SS_R/\sigma^2$ is $\chi^2_{\text{df}=k}$, and $SS_E/\sigma^2$ is $\chi^2_{\text{df}=n-k-1}$.

--

Our statistic is:

$$F_0=\frac{SS_R/k}{SS_E/(n-k-1)}=\frac{MS_R}{MS_E}$$

We reject $H_0$ if the computed value of the statistic $f_0>f_{1-\alpha,k,n-k}$

---

# Hypothesis Tests - ANOVA table

The process is summarized in an analysis of variance table, as follows:

.small[
| Source of Variation | Sum of Squares | df | Mean Squares | $F_0$ |
|----------------------|-----------------|:-----:|:------------:|---------------------|
| Regression | $SS_R$ | $k$ | $MS_R$ | $\frac{MS_R}{MS_E}$ |
| Error | $SS_E$ | $n-k-1$ | $MS_E$ |  |
| Total | $SS_T$ | $n-1$ |  |  |
]

--

## Adjusted $R^2$

We discussed $R^2 = 1-\frac{SS_E}{SS_T}$ however, as the number of parameters increases, the error always decreases.

To mitigate this phenomena, another measure is suggested: the adjusted $R^2$ statistic.

$R^2_{\text{adj}} = 1 - \frac{SS_E}{n-p}/\frac{SS_T}{n-1}$$

---

# Confidence Intervals

---

# Prediction Intervals

---

# How are Types of Variables Used in Regression?

---

# Multicolinearity

---

# Model Selection (Stepwise)

---

# Example 1 - Overfitting

---

# Example 2 - Outliers' Influence